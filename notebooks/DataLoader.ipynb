{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset # wraps an iterable around the dataset\n",
    "from torchvision import datasets    # stores the samples and their corresponding labels\n",
    "from torchvision.transforms import transforms  # transformations we can perform on our dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        current_directory = os.getcwd()\n",
    "        parent_directory = os.path.dirname(current_directory)\n",
    "        train_small_path = os.path.join(parent_directory, 'data', 'deepfake-ecg-small', 'train.csv')\n",
    "        xy = pd.read_csv(train_small_path)  # Skip the header row\n",
    "        \n",
    "        # QT\n",
    "        self.y = xy.iloc[:,[12]].values\n",
    "        patient_ids = xy.iloc[:,[1]].values \n",
    "\n",
    "        # ECG reports\n",
    "        self.x = np.empty((patient_ids.shape[0],8, 5000))\n",
    "        i = 0\n",
    "        # read each asc file\n",
    "        for [patient_id] in patient_ids:\n",
    "            asc_path = os.path.join(parent_directory, 'data', 'deepfake-ecg-small', 'train', str(patient_id)+'.asc')\n",
    "            matrix = self.read_file(asc_path)\n",
    "            self.x[i] = matrix\n",
    "            i = i + 1\n",
    "\n",
    "        # Size of the dataset\n",
    "        self.samples = xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve a sample from x and y based on the index\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return self.samples\n",
    "    \n",
    "    def read_file(self, filename):\n",
    "        # Read the file and extract the lines\n",
    "        with open(filename, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            # Initialize an empty matrix\n",
    "            matrix = np.empty((8, 5000))\n",
    "            # Iterate over each line and fill the matrix\n",
    "            for i, line in enumerate(lines):\n",
    "            # Split the line into individual values\n",
    "                values = line.split()\n",
    "                # Convert the values to integers and store them in the matrix\n",
    "                matrix[:, i] = np.array(values, dtype=int)\n",
    "\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG dataset\n",
    "dataset = ECGDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first data\n",
    "first_data = dataset[0]\n",
    "x, y = first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-127., -162., -142., ...,  -89.,  -39.,  -93.],\n",
       "       [  -1.,    0.,  -46., ...,  -18.,   22.,    5.],\n",
       "       [ -33.,   -8.,  -27., ...,   44.,   71.,   82.],\n",
       "       ...,\n",
       "       [ -92.,  -86.,  -87., ...,   67.,   89.,  105.],\n",
       "       [ -61.,  -67.,  -70., ...,   52.,   88.,   26.],\n",
       "       [   2.,  -29.,  -25., ...,   69.,  128.,  115.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([434], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "# It allows you to efficiently load and iterate over batches of data during the training or evaluation process.\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=16, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator object that allows you to iterate over the batches of data from the data loader.\n",
    "dataitter = iter(dataloader)\n",
    "\n",
    "# it retrieves the next batch of data from the iterator.\n",
    "data = dataitter.next()\n",
    "\n",
    "# split data\n",
    "x, y = data\n",
    "\n",
    "# print them\n",
    "print(x)\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
