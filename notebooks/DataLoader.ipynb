{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Setup and Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset # wraps an iterable around the dataset\n",
    "from torchvision import datasets    # stores the samples and their corresponding labels\n",
    "from torchvision.transforms import transforms  # transformations we can perform on our dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training \n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090 Ti'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataSetPrevious(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        current_directory = os.getcwd()\n",
    "        parent_directory = os.path.dirname(current_directory)\n",
    "        train_small_path = os.path.join(parent_directory, 'data', 'deepfake-ecg-small', 'train.csv')\n",
    "        xy = pd.read_csv(train_small_path)  # Skip the header row\n",
    "        \n",
    "        # QT\n",
    "        self.y = torch.tensor(xy['qt'].values, dtype=torch.float32)\n",
    "        patient_ids = xy['patid'].values\n",
    "\n",
    "        # ECG reports\n",
    "        self.x = []\n",
    "        # read each asc file\n",
    "        for patient_id in patient_ids:\n",
    "            asc_path = os.path.join(parent_directory, 'data', 'deepfake-ecg-small', 'train', str(patient_id)+'.asc')\n",
    "            ecg_data = np.loadtxt(asc_path)\n",
    "            ecg_tensor = torch.from_numpy(ecg_data)\n",
    "            ecg_tensor = ecg_tensor.float()\n",
    "            ecg_tensor = ecg_tensor.permute(1, 0)#.unsqueeze(2)\n",
    "            self.x.append(ecg_tensor)\n",
    "\n",
    "        # Size of the dataset\n",
    "        self.samples = xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve a sample from x and y based on the index\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return self.samples\n",
    "    \n",
    "    # def read_file(self, filename):\n",
    "    #     # Read the file and extract the lines\n",
    "    #     with open(filename, 'r') as file:\n",
    "    #         lines = file.readlines()\n",
    "    #         # Initialize an empty matrix\n",
    "    #         matrix = np.empty((8, 5000))\n",
    "    #         # Iterate over each line and fill the matrix\n",
    "    #         for i, line in enumerate(lines):\n",
    "    #         # Split the line into individual values\n",
    "    #             values = line.split()\n",
    "    #             # Convert the values to integers and store them in the matrix\n",
    "    #             matrix[:, i] = np.array(values, dtype=int)\n",
    "\n",
    "    #     return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        current_directory = os.getcwd()\n",
    "        self.parent_directory = os.path.dirname(current_directory)\n",
    "        train_small_path = os.path.join(self.parent_directory, 'data', 'deepfake-ecg-small', 'train.csv')\n",
    "        self.df = pd.read_csv(train_small_path)  # Skip the header row\n",
    "        \n",
    "        # QT\n",
    "        self.y = torch.tensor(self.df['qt'].values, dtype=torch.float32)\n",
    "\n",
    "        # Size of the dataset\n",
    "        self.samples = self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # file path\n",
    "        filename= self.df['patid'].values[index]\n",
    "        asc_path = os.path.join(self.parent_directory, 'data', 'deepfake-ecg-small', 'train', str(filename) + '.asc')\n",
    "        \n",
    "        ecg_signals = pd.read_csv( asc_path, header=None, sep=\" \") # read into dataframe\n",
    "        ecg_signals = torch.tensor(ecg_signals.values) # convert dataframe values to tensor\n",
    "        \n",
    "        ecg_signals = ecg_signals.float()\n",
    "        \n",
    "        # Transposing the ecg signals\n",
    "        ecg_signals = ecg_signals/6000 # normalization\n",
    "        ecg_signals = ecg_signals.t() \n",
    "        \n",
    "        qt = self.y[index]\n",
    "        # Retrieve a sample from x and y based on the index\n",
    "        return ecg_signals, qt\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return self.samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECG dataset\n",
    "dataset = ECGDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first data\n",
    "first_data = dataset[0]\n",
    "x, y = first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0212, -0.0270, -0.0237,  ..., -0.0148, -0.0065, -0.0155],\n",
       "        [-0.0002,  0.0000, -0.0077,  ..., -0.0030,  0.0037,  0.0008],\n",
       "        [-0.0055, -0.0013, -0.0045,  ...,  0.0073,  0.0118,  0.0137],\n",
       "        ...,\n",
       "        [-0.0153, -0.0143, -0.0145,  ...,  0.0112,  0.0148,  0.0175],\n",
       "        [-0.0102, -0.0112, -0.0117,  ...,  0.0087,  0.0147,  0.0043],\n",
       "        [ 0.0003, -0.0048, -0.0042,  ...,  0.0115,  0.0213,  0.0192]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(434.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "# It allows you to efficiently load and iterate over batches of data during the training or evaluation process.\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load whole dataset with DataLoader\n",
    "# shuffle: shuffle data, good for training\n",
    "# num_workers: faster loading with multiple subprocesses\n",
    "# !!! IF YOU GET AN ERROR DURING LOADING, SET num_workers TO 0 !!!\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "# # convert to an iterator and look at one random sample\n",
    "# dataiter = iter(train_loader)\n",
    "# data = next(dataiter)\n",
    "# features, labels = data\n",
    "# print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 5000]) torch.Size([32])\n",
      "torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataloader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x.dtype, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(8, 16, kernel_size=50, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(16 * 2500, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_classes = 1  # Number of output classes\n",
    "learning_rate = 0.000000001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(num_classes)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x39616 and 40000x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m batch_inputs, batch_labels \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m         \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m         outputs \u001b[39m=\u001b[39m model(batch_inputs)\n\u001b[1;32m      6\u001b[0m         loss \u001b[39m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[1;32m      8\u001b[0m         \u001b[39m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[25], line 15\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[1;32m     14\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n\u001b[1;32m     16\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x39616 and 40000x1)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_inputs, batch_labels in dataloader:\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss after every epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=40000, out_features=1000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1000, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=500, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "# nn.Module --> base class for all neural network modules\n",
    "class NeuralNetwork(nn.Module):\n",
    "    #network archirecture is defined in the init method\n",
    "    def __init__(self):\n",
    "        super().__init__()      #calls the __init__() method of the nn.Module pearent class \n",
    "        #( to ensure that the necessary setup and initialization from the parent class are performed.)\n",
    "        #This is important because the nn.Module class performs important bookkeeping tasks and sets up the internal state of the module.\n",
    "\n",
    "        #self.x are methods below. \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential( # allows to stack multiple layers in a sequential manner\n",
    "            nn.Linear(8*5000,1000 ),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    # method to define the forward pass computation of the model\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)   --> __call__ method is used to call the forward method (IMPORTANT)\n",
    "        #x = self.linear_relu_stack(x) this also can be used\n",
    "        x = self.flatten.forward(x)\n",
    "        logits = self.linear_relu_stack.forward(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)  # get the total number of samples in the dataset\n",
    "    model.train()   #sets the model in training mode (Stets the attribute named Training to True for the model instance) \n",
    "    #Dropout, batch normalization, etc. are used during training.\n",
    "\n",
    "    # iterates over the batches in the dataloader\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # moves the input data to the device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # compute prediction and loss --> Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        #Backpropagation\n",
    "        loss.backward() # compute the gradients of the model's parameters with respect to the loss function's output\n",
    "        optimizer.step()    #Update the models parameters an optimization algorithm\n",
    "        optimizer.zero_grad()   # Sets all the gradients to zero. If the gradients are not cleared they ll be accumilated.\n",
    "\n",
    "        # prints the progress of the training\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 153863.000000  [    0/10000]\n",
      "loss: 46131.101562  [ 3200/10000]\n",
      "loss: 23244.507812  [ 6400/10000]\n",
      "loss: 20522.169922  [ 9600/10000]\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/projects2/e17-4yp-compreh-ecg-analysis/minicondaInst/envs/test/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 14137.524414  [    0/10000]\n",
      "loss: 15390.925781  [ 3200/10000]\n",
      "loss: 11066.703125  [ 6400/10000]\n",
      "loss: 10521.293945  [ 9600/10000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 4095.266846  [    0/10000]\n",
      "loss: 7654.701172  [ 3200/10000]\n",
      "loss: 4049.261719  [ 6400/10000]\n",
      "loss: 4833.816406  [ 9600/10000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2947.991699  [    0/10000]\n",
      "loss: 4345.424805  [ 3200/10000]\n",
      "loss: 3229.486572  [ 6400/10000]\n",
      "loss: 3201.381104  [ 9600/10000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2689.642578  [    0/10000]\n",
      "loss: 2575.834229  [ 3200/10000]\n",
      "loss: 2501.104004  [ 6400/10000]\n",
      "loss: 3230.678223  [ 9600/10000]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1279.877808  [    0/10000]\n",
      "loss: 1154.874146  [ 3200/10000]\n",
      "loss: 1721.617188  [ 6400/10000]\n",
      "loss: 1112.253662  [ 9600/10000]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1334.359619  [    0/10000]\n",
      "loss: 1451.772705  [ 3200/10000]\n",
      "loss: 1268.673584  [ 6400/10000]\n",
      "loss: 1245.611450  [ 9600/10000]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1042.437256  [    0/10000]\n",
      "loss: 857.840454  [ 3200/10000]\n",
      "loss: 1068.422607  [ 6400/10000]\n",
      "loss: 1365.181641  [ 9600/10000]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 526.887817  [    0/10000]\n",
      "loss: 683.457642  [ 3200/10000]\n",
      "loss: 858.704285  [ 6400/10000]\n",
      "loss: 892.182129  [ 9600/10000]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 893.374756  [    0/10000]\n",
      "loss: 704.311096  [ 3200/10000]\n",
      "loss: 785.995117  [ 6400/10000]\n",
      "loss: 816.821960  [ 9600/10000]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 813.560852  [    0/10000]\n",
      "loss: 694.044434  [ 3200/10000]\n",
      "loss: 563.109985  [ 6400/10000]\n",
      "loss: 660.809387  [ 9600/10000]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 910.748169  [    0/10000]\n",
      "loss: 514.653564  [ 3200/10000]\n",
      "loss: 607.607422  [ 6400/10000]\n",
      "loss: 508.159637  [ 9600/10000]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 479.422913  [    0/10000]\n",
      "loss: 488.964630  [ 3200/10000]\n",
      "loss: 631.345032  [ 6400/10000]\n",
      "loss: 646.956543  [ 9600/10000]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 760.869019  [    0/10000]\n",
      "loss: 589.651306  [ 3200/10000]\n",
      "loss: 669.465698  [ 6400/10000]\n",
      "loss: 785.373169  [ 9600/10000]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 534.355469  [    0/10000]\n",
      "loss: 520.734253  [ 3200/10000]\n",
      "loss: 693.099609  [ 6400/10000]\n",
      "loss: 890.437988  [ 9600/10000]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 893.250916  [    0/10000]\n",
      "loss: 632.178345  [ 3200/10000]\n",
      "loss: 541.267517  [ 6400/10000]\n",
      "loss: 592.176147  [ 9600/10000]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 795.277344  [    0/10000]\n",
      "loss: 610.801147  [ 3200/10000]\n",
      "loss: 791.749634  [ 6400/10000]\n",
      "loss: 773.015137  [ 9600/10000]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 671.635681  [    0/10000]\n",
      "loss: 643.069580  [ 3200/10000]\n",
      "loss: 515.860718  [ 6400/10000]\n",
      "loss: 658.745605  [ 9600/10000]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 690.936646  [    0/10000]\n",
      "loss: 536.046631  [ 3200/10000]\n",
      "loss: 512.057251  [ 6400/10000]\n",
      "loss: 675.597656  [ 9600/10000]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 506.512268  [    0/10000]\n",
      "loss: 488.689697  [ 3200/10000]\n",
      "loss: 632.453369  [ 6400/10000]\n",
      "loss: 788.549072  [ 9600/10000]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 518.309143  [    0/10000]\n",
      "loss: 685.345764  [ 3200/10000]\n",
      "loss: 684.400146  [ 6400/10000]\n",
      "loss: 507.359589  [ 9600/10000]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 670.786316  [    0/10000]\n",
      "loss: 694.510742  [ 3200/10000]\n",
      "loss: 480.520874  [ 6400/10000]\n",
      "loss: 610.435547  [ 9600/10000]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 692.754150  [    0/10000]\n",
      "loss: 782.520630  [ 3200/10000]\n",
      "loss: 650.946655  [ 6400/10000]\n",
      "loss: 731.960571  [ 9600/10000]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 928.339233  [    0/10000]\n",
      "loss: 647.708069  [ 3200/10000]\n",
      "loss: 640.928589  [ 6400/10000]\n",
      "loss: 820.482788  [ 9600/10000]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 563.807678  [    0/10000]\n",
      "loss: 599.666199  [ 3200/10000]\n",
      "loss: 471.747375  [ 6400/10000]\n",
      "loss: 474.872711  [ 9600/10000]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 847.964600  [    0/10000]\n",
      "loss: 490.096100  [ 3200/10000]\n",
      "loss: 571.473877  [ 6400/10000]\n",
      "loss: 866.087402  [ 9600/10000]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 580.137451  [    0/10000]\n",
      "loss: 610.559631  [ 3200/10000]\n",
      "loss: 721.682312  [ 6400/10000]\n",
      "loss: 619.049072  [ 9600/10000]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 436.515961  [    0/10000]\n",
      "loss: 556.812988  [ 3200/10000]\n",
      "loss: 597.196899  [ 6400/10000]\n",
      "loss: 522.537964  [ 9600/10000]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 582.668457  [    0/10000]\n",
      "loss: 590.723816  [ 3200/10000]\n",
      "loss: 530.642456  [ 6400/10000]\n",
      "loss: 757.122070  [ 9600/10000]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 645.604858  [    0/10000]\n",
      "loss: 407.333130  [ 3200/10000]\n",
      "loss: 850.636230  [ 6400/10000]\n",
      "loss: 505.919128  [ 9600/10000]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 545.730103  [    0/10000]\n",
      "loss: 548.033569  [ 3200/10000]\n",
      "loss: 528.494629  [ 6400/10000]\n",
      "loss: 699.629517  [ 9600/10000]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 387.516083  [    0/10000]\n",
      "loss: 813.486572  [ 3200/10000]\n",
      "loss: 421.651367  [ 6400/10000]\n",
      "loss: 654.488037  [ 9600/10000]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 578.003662  [    0/10000]\n",
      "loss: 1014.754517  [ 3200/10000]\n",
      "loss: 766.245239  [ 6400/10000]\n",
      "loss: 634.644653  [ 9600/10000]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 483.515228  [    0/10000]\n",
      "loss: 661.846558  [ 3200/10000]\n",
      "loss: 656.171387  [ 6400/10000]\n",
      "loss: 553.461548  [ 9600/10000]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 873.208374  [    0/10000]\n",
      "loss: 539.816895  [ 3200/10000]\n",
      "loss: 433.282562  [ 6400/10000]\n",
      "loss: 578.057922  [ 9600/10000]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 761.324036  [    0/10000]\n",
      "loss: 561.935791  [ 3200/10000]\n",
      "loss: 703.895264  [ 6400/10000]\n",
      "loss: 425.487610  [ 9600/10000]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 797.663757  [    0/10000]\n",
      "loss: 544.498108  [ 3200/10000]\n",
      "loss: 645.008179  [ 6400/10000]\n",
      "loss: 497.880035  [ 9600/10000]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 594.701233  [    0/10000]\n",
      "loss: 355.275391  [ 3200/10000]\n",
      "loss: 414.418152  [ 6400/10000]\n",
      "loss: 602.603760  [ 9600/10000]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 543.133057  [    0/10000]\n",
      "loss: 668.568726  [ 3200/10000]\n",
      "loss: 500.374969  [ 6400/10000]\n",
      "loss: 558.263245  [ 9600/10000]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 500.243408  [    0/10000]\n",
      "loss: 607.324829  [ 3200/10000]\n",
      "loss: 741.821106  [ 6400/10000]\n",
      "loss: 524.627869  [ 9600/10000]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 388.976807  [    0/10000]\n",
      "loss: 730.665161  [ 3200/10000]\n",
      "loss: 498.102051  [ 6400/10000]\n",
      "loss: 565.202637  [ 9600/10000]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 529.809387  [    0/10000]\n",
      "loss: 685.413208  [ 3200/10000]\n",
      "loss: 689.918701  [ 6400/10000]\n",
      "loss: 559.949829  [ 9600/10000]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 569.737488  [    0/10000]\n",
      "loss: 537.335876  [ 3200/10000]\n",
      "loss: 589.825317  [ 6400/10000]\n",
      "loss: 551.920959  [ 9600/10000]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 580.718567  [    0/10000]\n",
      "loss: 594.021240  [ 3200/10000]\n",
      "loss: 601.396912  [ 6400/10000]\n",
      "loss: 828.005127  [ 9600/10000]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 459.468018  [    0/10000]\n",
      "loss: 404.491333  [ 3200/10000]\n",
      "loss: 637.518677  [ 6400/10000]\n",
      "loss: 708.582214  [ 9600/10000]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 598.811035  [    0/10000]\n",
      "loss: 708.292053  [ 3200/10000]\n",
      "loss: 630.371582  [ 6400/10000]\n",
      "loss: 537.913574  [ 9600/10000]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 527.559021  [    0/10000]\n",
      "loss: 493.668335  [ 3200/10000]\n",
      "loss: 445.354431  [ 6400/10000]\n",
      "loss: 511.228546  [ 9600/10000]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 648.441772  [    0/10000]\n",
      "loss: 396.076721  [ 3200/10000]\n",
      "loss: 457.188751  [ 6400/10000]\n",
      "loss: 552.267944  [ 9600/10000]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 393.371429  [    0/10000]\n",
      "loss: 553.020630  [ 3200/10000]\n",
      "loss: 603.316895  [ 6400/10000]\n",
      "loss: 469.236664  [ 9600/10000]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 429.830994  [    0/10000]\n",
      "loss: 407.798889  [ 3200/10000]\n",
      "loss: 734.247070  [ 6400/10000]\n",
      "loss: 664.825195  [ 9600/10000]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 651.813477  [    0/10000]\n",
      "loss: 719.551941  [ 3200/10000]\n",
      "loss: 479.934814  [ 6400/10000]\n",
      "loss: 529.423828  [ 9600/10000]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 513.231628  [    0/10000]\n",
      "loss: 522.971863  [ 3200/10000]\n",
      "loss: 571.606201  [ 6400/10000]\n",
      "loss: 512.520264  [ 9600/10000]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 508.561462  [    0/10000]\n",
      "loss: 555.149170  [ 3200/10000]\n",
      "loss: 542.270996  [ 6400/10000]\n",
      "loss: 445.701172  [ 9600/10000]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 511.692169  [    0/10000]\n",
      "loss: 449.854248  [ 3200/10000]\n",
      "loss: 432.684052  [ 6400/10000]\n",
      "loss: 561.253296  [ 9600/10000]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 563.677246  [    0/10000]\n",
      "loss: 624.869873  [ 3200/10000]\n",
      "loss: 434.149445  [ 6400/10000]\n",
      "loss: 650.644165  [ 9600/10000]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 564.554077  [    0/10000]\n",
      "loss: 487.755249  [ 3200/10000]\n",
      "loss: 653.217712  [ 6400/10000]\n",
      "loss: 577.043579  [ 9600/10000]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 675.920959  [    0/10000]\n",
      "loss: 506.525635  [ 3200/10000]\n",
      "loss: 511.053711  [ 6400/10000]\n",
      "loss: 630.064941  [ 9600/10000]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 686.699463  [    0/10000]\n",
      "loss: 570.970459  [ 3200/10000]\n",
      "loss: 294.876312  [ 6400/10000]\n",
      "loss: 752.478699  [ 9600/10000]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 423.929016  [    0/10000]\n",
      "loss: 593.282959  [ 3200/10000]\n",
      "loss: 720.767700  [ 6400/10000]\n",
      "loss: 550.542847  [ 9600/10000]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 415.149597  [    0/10000]\n",
      "loss: 368.819946  [ 3200/10000]\n",
      "loss: 354.559296  [ 6400/10000]\n",
      "loss: 626.733765  [ 9600/10000]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 661.148376  [    0/10000]\n",
      "loss: 628.885559  [ 3200/10000]\n",
      "loss: 645.567627  [ 6400/10000]\n",
      "loss: 590.479370  [ 9600/10000]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 436.564819  [    0/10000]\n",
      "loss: 716.818970  [ 3200/10000]\n",
      "loss: 566.247009  [ 6400/10000]\n",
      "loss: 671.280090  [ 9600/10000]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 519.810120  [    0/10000]\n",
      "loss: 559.316589  [ 3200/10000]\n",
      "loss: 418.587585  [ 6400/10000]\n",
      "loss: 850.922546  [ 9600/10000]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 424.862854  [    0/10000]\n",
      "loss: 667.272461  [ 3200/10000]\n",
      "loss: 547.305908  [ 6400/10000]\n",
      "loss: 464.299072  [ 9600/10000]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 499.357666  [    0/10000]\n",
      "loss: 386.236572  [ 3200/10000]\n",
      "loss: 620.685547  [ 6400/10000]\n",
      "loss: 625.050659  [ 9600/10000]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 497.897552  [    0/10000]\n",
      "loss: 495.172424  [ 3200/10000]\n",
      "loss: 530.822754  [ 6400/10000]\n",
      "loss: 448.619141  [ 9600/10000]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 439.418762  [    0/10000]\n",
      "loss: 391.549805  [ 3200/10000]\n",
      "loss: 493.434998  [ 6400/10000]\n",
      "loss: 517.052673  [ 9600/10000]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 423.928650  [    0/10000]\n",
      "loss: 549.612244  [ 3200/10000]\n",
      "loss: 423.306274  [ 6400/10000]\n",
      "loss: 570.746155  [ 9600/10000]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 354.108398  [    0/10000]\n",
      "loss: 702.549316  [ 3200/10000]\n",
      "loss: 433.381165  [ 6400/10000]\n",
      "loss: 620.093994  [ 9600/10000]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 455.411438  [    0/10000]\n",
      "loss: 565.693054  [ 3200/10000]\n",
      "loss: 528.326416  [ 6400/10000]\n",
      "loss: 410.082947  [ 9600/10000]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 572.334351  [    0/10000]\n",
      "loss: 366.368073  [ 3200/10000]\n",
      "loss: 777.228882  [ 6400/10000]\n",
      "loss: 598.129089  [ 9600/10000]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 414.494690  [    0/10000]\n",
      "loss: 413.206512  [ 3200/10000]\n",
      "loss: 663.427979  [ 6400/10000]\n",
      "loss: 424.415192  [ 9600/10000]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 462.781250  [    0/10000]\n",
      "loss: 476.399170  [ 3200/10000]\n",
      "loss: 511.739807  [ 6400/10000]\n",
      "loss: 613.252991  [ 9600/10000]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 338.624725  [    0/10000]\n",
      "loss: 404.664795  [ 3200/10000]\n",
      "loss: 608.933105  [ 6400/10000]\n",
      "loss: 407.929840  [ 9600/10000]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 766.186401  [    0/10000]\n",
      "loss: 496.211182  [ 3200/10000]\n",
      "loss: 442.727234  [ 6400/10000]\n",
      "loss: 277.648163  [ 9600/10000]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 457.360046  [    0/10000]\n",
      "loss: 535.709595  [ 3200/10000]\n",
      "loss: 355.192047  [ 6400/10000]\n",
      "loss: 498.561615  [ 9600/10000]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 436.311157  [    0/10000]\n",
      "loss: 453.997894  [ 3200/10000]\n",
      "loss: 327.963043  [ 6400/10000]\n",
      "loss: 395.372925  [ 9600/10000]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 353.520050  [    0/10000]\n",
      "loss: 411.958435  [ 3200/10000]\n",
      "loss: 445.704315  [ 6400/10000]\n",
      "loss: 654.034729  [ 9600/10000]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 330.439758  [    0/10000]\n",
      "loss: 443.593018  [ 3200/10000]\n",
      "loss: 367.314972  [ 6400/10000]\n",
      "loss: 603.569458  [ 9600/10000]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 618.801514  [    0/10000]\n",
      "loss: 539.181641  [ 3200/10000]\n",
      "loss: 476.221497  [ 6400/10000]\n",
      "loss: 401.335541  [ 9600/10000]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 500.798218  [    0/10000]\n",
      "loss: 377.893097  [ 3200/10000]\n",
      "loss: 535.201416  [ 6400/10000]\n",
      "loss: 251.524384  [ 9600/10000]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 427.038574  [    0/10000]\n",
      "loss: 307.400024  [ 3200/10000]\n",
      "loss: 341.070679  [ 6400/10000]\n",
      "loss: 671.013062  [ 9600/10000]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 666.135864  [    0/10000]\n",
      "loss: 575.179443  [ 3200/10000]\n",
      "loss: 604.089844  [ 6400/10000]\n",
      "loss: 519.796021  [ 9600/10000]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 463.301147  [    0/10000]\n",
      "loss: 461.707031  [ 3200/10000]\n",
      "loss: 402.471283  [ 6400/10000]\n",
      "loss: 300.012482  [ 9600/10000]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 432.798645  [    0/10000]\n",
      "loss: 691.748779  [ 3200/10000]\n",
      "loss: 581.700867  [ 6400/10000]\n",
      "loss: 530.878784  [ 9600/10000]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 700.317444  [    0/10000]\n",
      "loss: 558.789246  [ 3200/10000]\n",
      "loss: 454.991760  [ 6400/10000]\n",
      "loss: 556.877258  [ 9600/10000]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 633.413513  [    0/10000]\n",
      "loss: 487.960938  [ 3200/10000]\n",
      "loss: 486.433838  [ 6400/10000]\n",
      "loss: 513.713745  [ 9600/10000]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 448.815460  [    0/10000]\n",
      "loss: 525.864258  [ 3200/10000]\n",
      "loss: 470.209717  [ 6400/10000]\n",
      "loss: 571.182739  [ 9600/10000]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 605.840332  [    0/10000]\n",
      "loss: 450.101349  [ 3200/10000]\n",
      "loss: 376.007965  [ 6400/10000]\n",
      "loss: 549.471313  [ 9600/10000]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 378.620911  [    0/10000]\n",
      "loss: 697.037781  [ 3200/10000]\n",
      "loss: 417.627014  [ 6400/10000]\n",
      "loss: 413.094299  [ 9600/10000]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 320.127075  [    0/10000]\n",
      "loss: 368.481140  [ 3200/10000]\n",
      "loss: 439.706268  [ 6400/10000]\n",
      "loss: 421.893005  [ 9600/10000]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 688.577393  [    0/10000]\n",
      "loss: 406.331146  [ 3200/10000]\n",
      "loss: 805.488281  [ 6400/10000]\n",
      "loss: 600.241455  [ 9600/10000]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 480.684265  [    0/10000]\n",
      "loss: 571.733276  [ 3200/10000]\n",
      "loss: 400.216675  [ 6400/10000]\n",
      "loss: 473.241089  [ 9600/10000]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 426.135010  [    0/10000]\n",
      "loss: 377.327454  [ 3200/10000]\n",
      "loss: 436.299438  [ 6400/10000]\n",
      "loss: 414.813660  [ 9600/10000]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 508.969116  [    0/10000]\n",
      "loss: 436.304382  [ 3200/10000]\n",
      "loss: 641.414429  [ 6400/10000]\n",
      "loss: 526.664429  [ 9600/10000]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 530.562134  [    0/10000]\n",
      "loss: 359.330566  [ 3200/10000]\n",
      "loss: 492.593079  [ 6400/10000]\n",
      "loss: 472.545227  [ 9600/10000]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 384.728210  [    0/10000]\n",
      "loss: 328.826996  [ 3200/10000]\n",
      "loss: 686.513733  [ 6400/10000]\n",
      "loss: 466.433258  [ 9600/10000]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 411.813934  [    0/10000]\n",
      "loss: 682.687744  [ 3200/10000]\n",
      "loss: 521.212158  [ 6400/10000]\n",
      "loss: 333.811066  [ 9600/10000]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 402.291504  [    0/10000]\n",
      "loss: 379.239838  [ 3200/10000]\n",
      "loss: 510.811035  [ 6400/10000]\n",
      "loss: 419.863983  [ 9600/10000]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 386.462952  [    0/10000]\n",
      "loss: 501.597900  [ 3200/10000]\n",
      "loss: 385.643341  [ 6400/10000]\n",
      "loss: 699.332397  [ 9600/10000]\n",
      "Done!\n",
      "CPU times: user 3min 21s, sys: 32 s, total: 3min 53s\n",
      "Wall time: 10min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dataloader, model, loss_fn, optimizer)\n",
    "    #test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
