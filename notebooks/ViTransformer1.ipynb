{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer for ECG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Split image (ECG in our case) into patches and then embed them.\n",
    "\n",
    "    ECG --> 8,5000\n",
    "\n",
    "    Paramerters\n",
    "    ----------\n",
    "    img_size : int\n",
    "        Size of image (ECG) in pixels (samples).    (This is 1D 5000)\n",
    "\n",
    "    patch_size : int\n",
    "\n",
    "    in_chans : int\n",
    "        Number of input channels. (This is 8)\n",
    "\n",
    "    embed_dim : int\n",
    "        Embedding dimension.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    n_patches : int\n",
    "        Number of patches inside of our image.\n",
    "\n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches and their embedding.\n",
    "\n",
    "    \"\"\"\n",
    "    # This class is modified so that it works with 1D data.\n",
    "    def __init__(self, img_size=5000, patch_size=50, in_chans=8, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = img_size\n",
    "        patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size)\n",
    "\n",
    "        # embed_dim is the output channel size of the convolutional layer.\n",
    "        self.proj = nn.Conv1d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Shape is `(batch_size, in_chans, img_size)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Shape is `(batch_size, n_patches, embed_dim)`.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.proj(x) # (batch_size, embed_dim, n_patches)\n",
    "        # I dont think flatten is needed for 1D data.\n",
    "        #x = x.flatten(2) # flatten with 1st 2 dims intact\n",
    "        # (batch_size, embed_dim, n_patches) --> (batch_size, n_patches, embed_dim)\n",
    "        x = x.transpose(1,2)    # (batch_size, n_patches, embed_dim)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TestMac",
   "language": "python",
   "name": "testmac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
